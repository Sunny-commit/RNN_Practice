# ğŸ“ˆ RNN Practice - Recurrent Neural Networks for Time Series

A **comprehensive deep learning tutorial collection** demonstrating RNN architectures for time series prediction, natural language processing, and real-world applications using TensorFlow/Keras with multiple industry-relevant projects.

## ğŸ¯ Overview

This educational repository covers:
- âœ… Stock price prediction with LSTM
- âœ… Spam classification with embeddings
- âœ… Service load forecasting
- âœ… Time series analysis
- âœ… Word embeddings (GloVe integration)
- âœ… Multi-layer RNN architectures

## ğŸ—ï¸ Architecture

### Deep Learning Framework
- **Core**: TensorFlow 2.x, Keras Sequential/Functional API
- **RNN Types**: LSTM, GRU, Simple RNN
- **Embeddings**: Pre-trained GloVe (6B.50d)
- **Data Processing**: Pandas, NumPy, Scikit-learn
- **Visualization**: Matplotlib, TensorBoard

### Tech Stack
| Component | Technology |
|-----------|-----------|
| **Deep Learning** | TensorFlow 2.10+, Keras |
| **RNN Models** | LSTM, GRU, Bidirectional |
| **Embeddings** | GloVe 6B.50d (69 MB) |
| **Preprocessing** | Pandas, NumPy, scikit-learn |
| **Notebooks** | Jupyter for interactive learning |

## ğŸ“ Project Structure

```
RNN_Practice/
â”œâ”€â”€ Exercise Files/                                          # Main notebooks & data
â”‚
â”‚   â”œâ”€â”€ ğŸ“Š Time Series Prediction
â”‚   â”‚   â”œâ”€â”€ code_03_XX Predicting Stock Prices.ipynb        # Stock price forecasting
â”‚   â”‚   â”œâ”€â”€ FB-stock-prices.csv                             # Facebook stock data
â”‚   â”‚   â”œâ”€â”€ code_05_XX Forecasting Service Loads.ipynb      # Load prediction (680 KB)
â”‚   â”‚   â””â”€â”€ requests_every_hour.csv                         # Hourly request data
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ NLP & Classification
â”‚   â”‚   â”œâ”€â”€ code_07_XX Spam Classification with Embeddings.ipynb  # Spam detection
â”‚   â”‚   â””â”€â”€ Spam-Classification.csv                         # Dataset (168 KB)
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ”¤ Pre-trained Embeddings
â”‚       â””â”€â”€ glove.6B.50d.txt.zip                           # GloVe word vectors (69 MB)
â”‚
â”œâ”€â”€ CONTRIBUTING.md                                          # Contribution guidelines
â”œâ”€â”€ LICENSE                                                  # Open source license
â”œâ”€â”€ NOTICE                                                   # Attribution
â””â”€â”€ README.md
```

## ğŸ“ Core Projects

### Project 1: Stock Price Prediction (LSTM)

**File**: `code_03_XX Predicting Stock Prices.ipynb` (174 KB)

**Objective**: Predict future Facebook stock prices using historical data

```python
# Architecture
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

model = Sequential([
    LSTM(50, activation='relu', input_shape=(look_back, 1)),
    Dropout(0.2),
    LSTM(50, activation='relu'),
    Dropout(0.2),
    Dense(25, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)
```

**Data Pipeline**
```
Facebook Stock Prices (FB-stock-prices.csv)
    â†“
[Data Preprocessing]
â”œâ”€â”€ Load historical data
â”œâ”€â”€ Normalize values (MinMaxScaler)
â””â”€â”€ Create sequences (look-back window)
    â†“
[Train/Test Split]
â”œâ”€â”€ 80% training data
â””â”€â”€ 20% testing data
    â†“
[LSTM Training]
â”œâ”€â”€ Input shape: (sequences, time steps, features)
â”œâ”€â”€ Multi-layer LSTM with dropout
â”œâ”€â”€ MSE loss optimization
â””â”€â”€ Early stopping to prevent overfitting
    â†“
[Predictions]
â”œâ”€â”€ Forecast next price
â”œâ”€â”€ Calculate RMSE error
â””â”€â”€ Visualize predictions vs actual
```

**Key Learnings**
- Sequential data handling
- LSTM cell architecture
- Temporal pattern recognition
- Hyperparameter tuning for financial data

### Project 2: Service Load Forecasting

**File**: `code_05_XX Forecasting Service Loads.ipynb` (680 KB - largest notebook)

**Objective**: Predict server/service loads for infrastructure planning

```python
# Time Series Forecasting Architecture
from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed

model = Sequential([
    # Encoder
    LSTM(64, input_shape=(look_back, features), return_sequences=True),
    LSTM(32),
    
    # Decoder
    RepeatVector(forecast_horizon),
    LSTM(32, return_sequences=True),
    LSTM(64, return_sequences=True),
    TimeDistributed(Dense(features))
])

model.compile(optimizer='adam', loss='mse')
```

**Dataset**: `requests_every_hour.csv`
- Hourly service requests
- Pattern analysis (daily/weekly cycles)
- Peak load prediction
- Anomaly detection

**Applications**
- Auto-scaling infrastructure
- Capacity planning
- Cost optimization
- SLA compliance

### Project 3: Spam Classification with Embeddings

**File**: `code_07_XX Spam Classification with Embeddings.ipynb` (16.4 KB)

**Objective**: Classify emails/messages as spam using word embeddings

```python
# NLP Pipeline with Pre-trained Embeddings
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load GloVe embeddings
embeddings_index = {}
with open('glove.6B.50d.txt') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Create embedding matrix
embedding_dim = 50
embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

# Model with pre-trained embeddings
model = Sequential([
    Embedding(vocab_size, embedding_dim, 
              weights=[embedding_matrix], 
              trainable=False),
    LSTM(32),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

**Data Pipeline**
```
Spam Dataset (Spam-Classification.csv - 168 KB)
    â†“
[Text Preprocessing]
â”œâ”€â”€ Tokenization (split into words)
â”œâ”€â”€ Vocabulary building
â”œâ”€â”€ Sequence padding
â””â”€â”€ Integer encoding
    â†“
[GloVe Embeddings]
â”œâ”€â”€ Pre-trained word vectors (50-dimensional)
â”œâ”€â”€ Vocabulary mapping
â””â”€â”€ Embedding matrix creation
    â†“
[LSTM Classification]
â”œâ”€â”€ Sequential text encoding
â”œâ”€â”€ Long-term dependency capture
â”œâ”€â”€ Binary classification
â””â”€â”€ Dropout regularization
    â†“
[Evaluation]
â”œâ”€â”€ Accuracy (%95+)
â”œâ”€â”€ Precision/Recall
â”œâ”€â”€ Confusion matrix
â””â”€â”€ Feature importance
```

**Key Features**
- Transfer learning with pre-trained embeddings
- Significant dimensionality reduction
- Real-time text classification
- Sequence-aware architecture

## ğŸš€ Installation & Setup

### Prerequisites
```bash
Python 3.7+
Jupyter Notebook/Lab
GPU support (recommended)
```

### Step-by-Step Setup

```bash
# 1. Clone Repository
git clone https://github.com/Sunny-commit/RNN_Practice.git
cd RNN_Practice

# 2. Create Virtual Environment
python -m venv env
source env/bin/activate  # Windows: env\Scripts\activate

# 3. Install Dependencies
pip install tensorflow keras numpy pandas scikit-learn matplotlib jupyter

# 4. Download Pre-trained Embeddings
cd "Exercise Files"
unzip glove.6B.50d.txt.zip
cd ..

# 5. Launch Jupyter
jupyter notebook
```

## ğŸ“Š Project Progression

| Project | Difficulty | Notebook Size | Duration | Topics |
|---------|-----------|--------------|----------|--------|
| 1. Stock Prediction | â­â­ | 174 KB | 2-3 hrs | LSTM, time series |
| 2. Service Load | â­â­â­ | 680 KB | 4-5 hrs | Encoder-decoder, forecasting |
| 3. Spam Classification | â­â­ | 16 KB | 1-2 hrs | Embeddings, classification |

## ğŸ§  Deep Learning Concepts Covered

### RNN Architecture
```
Input Sequence (xâ‚, xâ‚‚, xâ‚ƒ, ..., xâ‚œ)
    â†“
[RNN Cell]
â”œâ”€â”€ Hidden state: h = tanh(Wâ‚•â‚•Â·h + Wâ‚“â‚•Â·x + bâ‚•)
â”œâ”€â”€ Output: y = Wâ‚•áµ§Â·h + báµ§
â””â”€â”€ Recurrent connection: h_t uses h_(t-1)
    â†“
[LSTM Variant]
â”œâ”€â”€ Forget gate: fâ‚œ = Ïƒ(WfÂ·[h_(t-1), xâ‚œ])
â”œâ”€â”€ Input gate: iâ‚œ = Ïƒ(Wáµ¢Â·[h_(t-1), xâ‚œ])
â”œâ”€â”€ Cell state: Câ‚œ = fâ‚œ * C_(t-1) + iâ‚œ * CÌƒâ‚œ
â”œâ”€â”€ Output gate: oâ‚œ = Ïƒ(Wâ‚’Â·[h_(t-1), xâ‚œ])
â””â”€â”€ Hidden state: hâ‚œ = oâ‚œ * tanh(Câ‚œ)
    â†“
Output Sequence (Å·â‚, Å·â‚‚, Å·â‚ƒ, ..., Å·â‚œ)
```

### Word Embeddings
```
Text â†’ Tokenization â†’ Integer Sequence â†’ Embedding Lookup â†’ Dense Vectors
"Good email" â†’ [42, 156] â†’ [[0.25, -0.1, ...], [0.8, 0.2, ...]]
```

### Time Series Patterns
- Trend: Long-term directional movement
- Seasonality: Regular repeating patterns
- Autocorrelation: Dependencies on previous values
- Stationarity: Constant statistical properties

## ğŸ“ˆ Expected Results

### Stock Prediction Accuracy
- RMSE: $2-5 per share (depending on volatility)
- Directional accuracy: 55-60%
- Real-world limitaions: Market noise, external events

### Load Forecasting Accuracy
- MAPE: 5-10%
- Peak load prediction: 90%+ accuracy
- Anomaly detection: 85%+ precision

### Spam Classification
- Accuracy: 95%+
- Precision: 94%+
- Recall: 96%+

## ğŸ”§ Key Implementation Details

### Hyperparameter Tuning
```python
# Experimentation parameters
look_back = 60              # Sequence length
batch_size = 32             # Training batch
epochs = 100                # Training iterations
dropout_rate = 0.2          # Regularization
learning_rate = 0.001       # Optimizer rate
```

### Data Normalization
```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)
# Normalization critical for LSTM convergence
```

### Early Stopping
```python
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)
```

## ğŸ¯ Real-world Applications

**Finance**
- Stock price forecasting
- Portfolio optimization
- Risk assessment

**Infrastructure**
- Server load prediction
- Auto-scaling triggers
- Capacity planning

**Security**
- Spam/phishing detection
- Anomaly detection
- Threat identification

**Healthcare**
- Patient monitoring  
- Disease progression
- Treatment planning

## ğŸ“ Learning Objectives

After completing this project, you'll understand:
- âœ… RNN & LSTM architecture
- âœ… Time series analysis & forecasting
- âœ… Transfer learning with embeddings
- âœ… Sequence-to-sequence models
- âœ… Production ML pipelines
- âœ… Model evaluation metrics

## ğŸ› ï¸ Advanced Topics

### GRU (Gated Recurrent Unit)
```python
from tensorflow.keras.layers import GRU

model = Sequential([
    GRU(32, input_shape=(look_back, features)),
    Dense(1)
])
# Simpler than LSTM, similar performance
```

### Bidirectional Processing
```python
from tensorflow.keras.layers import Bidirectional

model = Sequential([
    Bidirectional(LSTM(32, input_shape=(look_back, features))),
    Dense(1)
])
# Process sequences forward AND backward
```

### Attention Mechanism
```python
from tensorflow.keras.layers import Attention

# Allows model to focus on relevant parts
query = LSTM(32)(encoder_input)
attention = Attention()([query, encoder_output])
```

## ğŸ“š References & Resources

- [Keras RNN Documentation](https://keras.io/api/layers/recurrent_layers/)
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
- [Time Series Forecasting Best Practices](https://machinelearningmastery.com/)

## ğŸ¤ Contributing

Contributions welcome:
- Additional notebooks (sentiment analysis, machine translation)
- Performance optimizations
- TensorFlow 2.x updates
- Documentation improvements

## ğŸ“„ License

Licensed under MIT License - See LICENSE file

## ğŸŒŸ Key Takeaways

âœ… Comprehensive RNN learning path
âœ… Industry-relevant applications  
âœ… Pre-trained embeddings integration
âœ… Time series best practices
âœ… Production-ready code patterns
âœ… Multiple architectural variations
âœ… Real-world datasets included

## ğŸ“§ Support

For questions or issues: Check CONTRIBUTING.md or open GitHub Issue

---

**Recommended Learning Order**:
1. Start: Spam Classification (simplest)
2. Progress: Stock Price Prediction (medium)
3. Advanced: Service Load Forecasting (most complex)
